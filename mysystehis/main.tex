\documentclass[a4paper, 13pt]{extarticle}
\usepackage{graphicx}
\usepackage[margin=2.5cm]{geometry}
\graphicspath{{images/}}
\usepackage{amsmath}
\usepackage{titlesec}
\titlelabel{\thetitle.\quad}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{parskip}
\usepackage{helvet}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage{mathptmx}
\usepackage{setspace}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{pgf,tikz,pgfplots}
\usepackage{mathrsfs}
\usepackage{xfrac}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{textcomp}
\usetikzlibrary{arrows}



\renewcommand{\baselinestretch}{1.5} 
\begin{document}


\begin{titlepage}

  \newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

  \center % Center everything on the page

%	HEADING SECTION

  %\textsc{\LARGE University Name}\\[1.5cm] % Name of your university/college
  \text{\Large UNIVERSITY OF SCIENCE AND TECHNOLOGY OF HANOI}\\[0.3cm] % Major heading such as course name
  \textsc{\bfseries UNDERGRADUATE SCHOOL}\\[1.2cm] % Minor heading such as course title
  \includegraphics[scale = 0.1]{usth.png}\\[1cm]
  \text{ \large Research and Development}\\[0.5cm]
    {\LARGE \bfseries BACHELOR THESIS}  \\ [1cm]
    \text{\large By}\\[0.3cm]
  \text{ \large Bui Vu Huy}\\[0.3cm]
  \text{ \large USTHBI7-082}\\[0.3cm]
  \text{\large Information and Communication Technology }\\[0.5cm]
  
  %	TITLE SECTION
  
  \vspace{1 cm}
  \HRule\\[0.5cm]

  \textbf{\fontsize{18}{20} \bfseries Virtual World Development using Unity Engine}\\[0.3cm] % Title of your document
    \HRule\\[0.5cm]

  
  %	AUTHOR SECTION
  
  \text{\large Supervisor:    Dr. Nguyen Hoang Ha}\\
  
  %	DATE SECTION
  
  \vspace{2.5 cm}
  \textbf{\large Hanoi, \today}\\[3cm] % Date, change the \today to a set date if you want to be precise
  
  
  \vfill % Fill the rest of the page with whitespace
  
\end{titlepage}
\newpage
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
\tableofcontents

\newpage

\newpage
\section*{\color{cyan}\Large Acknowledgements}

First of all, i would like to thanks Dr.Nguyen Hoang Ha for giving me 3 months for using the Unity Engine to develop the Virtual World. Also, thank you for supporting me during this internship. 

I also thanks for the USTH ICT Lab for giving me a opportunities to work in a places like in a professional company. 

Finally, I'd like to give special thanks to the guys, girls who made free 3d models, characters in the Unity assets store, so that i can download, use them for my project.
\newpage
 
 \newpage
 \appendix
  \renewcommand{\thesubsection}{\Alph{section}}
 \renewcommand{\thesubsection}{\arabic{subsection}}
\section{\Large Introduction} 
Nowsadays, the technology is getting better and better, so the human life are becoming more and more convenient. With that, people's entertainment needs are also increasing. However, to satisfy people's needs, also to keep up with this rapid development of technology, a virtual world is also a type of entertainment that can satisfy people's need. \\[0.35cm]  A virtual world is a computer-based environment that the user can interact with each other in the world. In general, the virtual world usually using 3-dimensional graphic, with 3D models, which can make people feel like they're in the real world. Virtual worlds allow for multi user to communicate, and a 3D video single player game like Elder Scroll: Blade and Skyrim can still be consider as the virtual world. \\[0.35cm] In general term, there's still no generally accepted definition for virtual world, it supports varying degrees of play and gaming. These are some uses of term: MMOGs game: large number of players within a game, and RPG game, etc... \\[0.35cm] To create a virtual world, there're some types of engine that allow you to make like Unity, Unreal Engine, blender, etc... \\[0.35cm] From one of those engines, Unity is the most popular choice for many people, from beginner to expert. \\[0.35cm] Unity is a cross-platform game engine developed by Unity Technologies, first announced and released in June 2005 at Apple Inc.'s Worldwide Developers Conference as a Mac OS X-exclusive game engine. As of 2018, the engine had been extended to support more than 25 platforms. The engine can be used to create three-dimensional, two-dimensional, virtual reality, and augmented reality games, as well as simulations and other experiences. \\[0.35cm]
 In my opinion, first of all, it has tons of online tutorials to be found, the document with clean format, so a good step for beginners. With it's very intuitive design, C\# language make it easier to use or learn, like you can call other script within a script. Also, with great community, when you have a problem, they'll have you everywhere and every time. Unity has clean API, which is easy to use, understand, implement in C\# code, the application works very well on windows, Linux, android or iOS. Lastly, unlike other engines, the unity has asset store with lots of free assets for everyone to use, while the other ones only have few free assets. \\[0.35cm]In this work, i'll focus on creating the virtual world using unity engine, in particular, it's an RPG game, where you as a player can walk around, interact with other NPC model in the world, or attack some enemy that get in your way. 
  

 To create the virtual world with Unity, first, we'll need to create the terrain by the tools Unity provides for us. After that, we can create my own model and then just simply drag and drop the model into the scene, if not, there're several websites i can get free 3d model from like turbosquid or free3d. For the Lighting and the shadow, Unity also has build-in tool just like other engines, which can help me create the light and shadow easily, adjust it as i want. Furthermore, to make things look good, we can apply some textures to all the objects, prefabs. We can find the texture on google, choose the suitable one for a specific object, and then just simply drag and drop them on that object, or making our own texture. Finally, in order to move around or interact with the object, a camera and player controller should be added into the scene, however, Unity already has the FPScontroller prefab which can be found in the standard assets, we can use it directly without doing anything, but it's still possible to create the character controller from scratch with C\# code.
 \section{\Large Objective} 
 	In this project, since it's about building an RPG game, which is also a type of virtual world, I'll only focus on how to make the player move around the world, is able to interact with other objects around him. In this game, there may be some enemies that get in the way, the player should eliminate them which hurt the player. 
 \section{\Large Aim} 
 	The aim is not necessarily on destroying the enemies, but focus on completing 
 	the quest that villagers in the village gave the player. When finishing a quest, the player should return back to the village, talk to the villagers that you're finished. Beside that, the game should also have an online chat tab , which you can chat with other players who are connecting to the same network while playing game, they may help you finishing the quest faster. 

 
 

 \titleformat{\paragraph}
 {\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
 \titlespacing*{\paragraph}
 {0pt}{1.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
 
 \vspace{0.35 cm}
  \section{\Large Programs, Materials and Methods}
  	\subsection{External programs used}
  		\subsubsection{3DS Max}
 		 3DS Max, a program developed by Autodesk, previously called 3D Studio Max, is a 3D computer graphics program for making 3D animations, models , images...
 		 This program was used in this project in order to edit UV mapping of some models, correct the position of the texture. 
 		 \subsubsection{Photoshop}
 		 Photoshop is a photo editing and graphic design software. It is developed by Adobe Systems for MacOS and Windows. It's not like other graphic design or photo editing softwares, it can create normal map, height map, occulusion map from a single texture. With this, this software will help me with the technique bump mapping, so the game's texture will look more realistic. 
 		 \subsubsection{Visual Studio}
 		 Visual studio is an IDE (integrated development environment ) from Microsoft. It's used to develop computer programs, as well as websites, supports many programming languages such as : C++, C\#, JavaScript, etc... 
 		 This is the main software of this project, because i'm using this one to write scripts in order to make the game works.
 		 When installing unity 5, normally it's shipped with visual studio community which is free for everyone. Previously, it's shipped with the monodevelop, but now the monodevelop is discontinued, no longer support unity, replaced with visual studio community as it's more powerful than monodevelop, it support MacOS and windows, gives you a cloud storage for saving like one drive(when you project goes wrong), which monodevelop doesn't have.  
 		
 		 
 		 
 		 \subsection{Materials}
 		 \subsubsection{DirectX}
 		 DirectX is the collection of API (or  application programming interfaces), also made by Microsoft, for handling task related to multimedia. It contains these APIs such as Direct3D, DirectDraw, DirectMusic, DirectSound .... , especially for game programming. \\[0.15cm] The DirectX version use in this project here is Direct X11 because from unity 4.x or higher, the engine tend to use more cpu cores, and with Direct3D 11, it has improved multi-threading support so it can utilize multi-core better. Without Direct X, the engine cannot simulate the light in the game or even run. 
 		 \subsubsection{A dedicated graphic card}
 		 A graphic card with direct x11 compatible is compulsory for Unity. Because unity comes with MSAA support, to improve image, textures quality, which direct x10 or lower doesn't support. Integrated GPU is fine also as long as it's powerful enough to run unity programs, games and have direct x11 compatible. 
 		 
 		 \subsection{Method}  
 		 \subsubsection{Overview Diagram}
 		 - In this section, i'd like to introduce my overview diagram about the concept of creating a virtual world: \par
 		 
 		 	\begin{figure}[h]
 		 		\centering
 		 		\includegraphics[width=0.59\columnwidth]{Overview_diagram.png}
 		 		\captionof{figure}{Overview Diagram}
 		 		\label{fig:Overview1}
 		 	\end{figure}
 		 
 		 
 		 \newpage
 		  \subsubsection{Details}
 		  \paragraph{Preparation}
 		  Unity can be download from the \href{https://store.unity.com/}{offical website}. I prefer Unity personal version because it's free, for everyone. As i mentioned before, to be able to run Unity, and create a new project, you'll need at least direct x11 for simulate lighting, shadows on the scene. To create a terrain, which is the base of the virtual world, all i have to do is go to game object -> 3D Object -> Terrain. The terrain width and length is only set to 500x500, which i think is big enough for this project.  \begin{figure}[h]
 		  	\centering
 		  	\begin{minipage}{.4\textwidth}
 		  		\centering
 		  		\includegraphics[width=1\linewidth]{intructions/1.png}
 		  		\captionof{figure}{Create terrain}
 		  		\label{fig:test2}
 		  	\end{minipage}
 		  	\begin{minipage}{.4\textwidth}
 		  		\centering
 		  		\includegraphics[width=1.4\linewidth]{intructions/2.png}
 		  		\captionof{figure}{Properties}
 		  		\label{fig:test3}
 		  	\end{minipage}
 		  \end{figure}
 	  \\[0.15cm]
 	  From the Inspector section, there're 4 sections: Paint texture, paint trees, paint details. Each of these sections has painting tool with 20 brush presets, resizable brush size, opacity. For the textures, you can use any texture you want for the terrain, for example i use this texture from this website: \href{https://www.textures.com/download/grass0153/48704}{https://www.textures.com/download/grass0153/48704}. The same applied for painting details, but for painting trees, the material for this one is 3D model, however it can be found in the Standard Assets which comes with Unity when installed. 
 	   \begin{figure}[h]
 	   	 \centering
 	   	 \begin{minipage}{1\textwidth}
 	   	 	\centering
 	   	 	\includegraphics[width=0.75\linewidth]{intructions/3.png}
 	   	 	\captionof{figure}{Sections}
 	   	 	\label{fig:test4}
 	   	 \end{minipage}
 		 \end{figure}  
 		 
 	Besides, in order to make the terrain looks better, the user can also raise or lower the terrain to create mountains, river. After i finish creating the terrain, all i have to do is drag and drop other objects/prefabs into the scene, put the textures corresponding to each object, while these prefabs/objects can be found on unity assets store or from free3d website as i mentioned from the introduction. 
 	 \paragraph{Player}
 	 To be able to interact with the objects, NPCs, a player should be added into the scene. For example, i use this little robot for my player, this model is free to download, use: \href{https://free3d.com/3d-model/bb8-35865.html}{https://free3d.com/3d-model/bb8-35865.html}.
 	 
 	 \begin{figure}[h]
 	 	\centering
 	 	\begin{minipage}{1\textwidth}
 	 		\centering
 	 		\includegraphics[width=0.75\linewidth]{intructions/4.png}
 	 		\captionof{figure}{Character example}
 	 		\label{fig:test5}
 	 	\end{minipage}
 	 \end{figure}
 	  
 	 Initially, the player will not move without any scripts attached to it, so i write a simple C\# script to control the player using visual studio and then attach the script to the player. Although there's a character controller prefab in the Unity's standard asset, which can be used, controlled instantly without doing anything. In this project, i'll not use character controller prefab, so that i can easily customize my character controller script later. \\[0.25cm] By default, when initializing a unity project, there's already a camera called "Main camera", which is what the user can see from the game view in the hierarchy. This camera will not follow my character as i move, but i'll explain how to make the camera follow my character later.  
 	 \\[0.25cm] From now, i'm going to explain how can i implement character controller to move my character. Firstly, i have to find the input by go to Edit -> Project Settings -> Input. In the input section, as you can see, the Unity already implement 2 inputs named : "Horizontal" , "Vertical", along with Negative Button and Positive Button, and i can also add my own custom input in project setting. This is where i can move my character left and right, forward and backward. After finding the input, i can get the input by using: 
 	 \lstdefinestyle{customc}{
 	 	belowcaptionskip=1\baselineskip,
 	 	breaklines=true,
 	 	frame=L,
 	 	xleftmargin=\parindent,
 	 	language=C,
 	 	showstringspaces=false,
 	 	basicstyle=\footnotesize\ttfamily,
 	 	keywordstyle=\bfseries\color{green!40!black},
 	 	commentstyle=\itshape\color{purple!40!black},
 	 	identifierstyle=\color{blue},
 	 	stringstyle=\color{orange},
 	 }
  
  \lstset{style=customc, basicstyle=\fontsize{7.5}{4}\ttfamily, language=C}
 	 \begin{lstlisting}
 	  Vector2 input = new Vector2(Input.GetAxis("Horizontal"), 
 	  Input.GetAxis("Vertical"));
 	 \end{lstlisting}
 	 Next, i have to normalize the input vector to see which direction the player have to move , face correctly. To be more specify, let's consider two trigonometry below:
 	
 	 		\definecolor{qqwuqq}{rgb}{0.,0.39215686274509803,0.}
 	 		\definecolor{ffqqqq}{rgb}{1.,0.,0.}
 	 		\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
 	 		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=0.5cm,y=0.5cm]
 	 		\clip(-17.62,-11.46) rectangle (13.08,6.28);
 	 		\draw [shift={(-10.3,-3.27)},line width=0.5pt,color=qqwuqq,fill=qqwuqq,fill opacity=0.10000000149011612] (0,0) -- (0.4233601239113539:0.6) arc (0.4233601239113539:49.49045192111633:0.6) -- cycle;
 	 		\draw [line width=0.5pt] (-14.36,-3.3)-- (-6.24,-3.24);
 	 		\draw [line width=0.5pt] (-10.3,-3.27) circle (2.030055417962771cm);
 	 		\draw [line width=0.5pt] (-10.3,-3.27)-- (-6.24,-3.24);
 	 		\draw [line width=0.5pt] (-10.33,0.79)-- (-10.27,-7.33);
 	 		\draw [line width=2.8pt,color=ffqqqq] (-10.3,-3.27)-- (-7.662654488417639,-0.1831069580342808);
 	 		\draw [line width=0.5pt] (-7.662654488417639,-0.1831069580342808)-- (-7.639990169721227,-3.2503447549486797);
 	 		\draw (-10.74,1.68) node[anchor=north west] {90};
 	 		\draw [line width=0.5pt] (1.77,-3.3) circle (2.030055417962771cm);
 	 		\draw [line width=0.5pt] (-2.25,-3.31)-- (5.87,-3.25);
 	 		\draw (-5.82,-2.98) node[anchor=north west] {0};
 	 		\draw (-15.86,-3.14) node[anchor=north west] {180};
 	 		\draw (-10.94,-7.76) node[anchor=north west] {270};
 	 		\draw (1.46,1.66) node[anchor=north west] {0};
 	 		\draw (6.42,-2.92) node[anchor=north west] {90};
 	 		\draw (1.28,-7.96) node[anchor=north west] {180};
 	 		\draw (-3.74,-3.02) node[anchor=north west] {270};
 	 		\draw [line width=2.8pt,color=ffqqqq] (1.77,-3.3)-- (1.7404368698694395,0.7600032045968739);
 	 		\draw [line width=0.5pt] (1.77,-3.3)-- (1.7995631301305606,-7.3600032045968735);
 	 		\begin{scriptsize}
 	 		\draw [fill=uuuuuu] (-10.3,-3.27) circle (2.0pt);
 	 		\draw[color=black] (-8.94,-3.55) node {$X$};
 	 		\draw[color=black] (-7.32,-1.87) node {$Y$};
 	 		\draw[color=qqwuqq] (-9.22,-2.87) node {$\alpha$};
 	 		\draw [fill=uuuuuu] (1.77,-3.3) circle (2.0pt);
 	 		\end{scriptsize}
 	 		\end{tikzpicture}
 	 	\\[0.02cm]
 	 	From the left one above, suppose that the red line is the input direction, X and Y are vertical and horizontal component. Now, i have to find the angle {$\alpha$} which is the direction the character will rotate, calculated as: 
 	 	\\[-0.5cm]
 	 	\[\alpha = arctan(\frac{Y}{X})\]
 	 	In Unity, however, if my character is facing forward, then it has the rotation of 0 (as seen from the right one), when facing right it has the rotation of 90 degree and so on. Let's call the rotation of character in unity is r, when looking at the two circle, it's clearly that r is calculated as: \\[-0.5cm]
 	 	 \[r = 90 - \alpha; \hspace{0.8cm} or \hspace{1.0cm} r = arctan(\frac{X}{Y})\] 
 	 	By C\# code:
 	 	\begin{lstlisting}
 	 	float targetRotation = Mathf.Atan2(inputDir.x, inputDir.y) * Mathf.Rad2Deg;
 	 	transform.eulerAngles = Vector3.up * Mathf.SmoothDampAngle(transform.eulerAngles.y, targetRotation, ref turnSmoothVelocity, turnSmoothTime);
 	 	\end{lstlisting}
 	 	
 	 	Where Mathf.SmoothDampAngle is the function that can gradually change an angle given in degrees towards a desired angle by turnSmoothTime (in second). The reason i use Vector3.up is because the character won't rotate up or fly straight to the sky.  \\[0.15cm] After that, to be able to move, transform.forward and Controller.Move is used here to move the character by a certain amount in the world space everytime when i press a button. \\[0.15cm] Finally, to finish setting up my character controller script, i have also added gravity, ability to jump for my character. From 10th grade, the equation to calculate the falling speed is defined as: 
 	 	\\[-0.5cm]
 	 	 \[v = \sqrt{2gh}\]
 	 	 where g is the gravity and h is the height the character begin to fall. As there's no air resistance in the game, this equation can be applied directly to the character through the script. For the ability to jump:
 	 	 \begin{lstlisting}
 	 	 if (controller.isGrounded)
 	 	 {
 	 	 	float jumpVelocity = Mathf.Sqrt(-2 * gravity * jumping);
 	 	 	VelocityY = jumpVelocity;
 	 	 }
 	 	 Vector3 velocity = transform.forward * currentSpeed + Vector3.up * VelocityY;
 	 	 controller.Move(velocity * Time.deltaTime);
 	 	 
 	 	 if (controller.isGrounded)
 	 	 {
 	 	 	VelocityY = 0;  
 	 	 }
 	 	 \end{lstlisting}
 	 	 Where VelocityY is defined as the falling speed equation above. As soon the character hit the ground, the character won't be able to fall anymore, so i have to assign VelocityY = 0 when controller.isGrounded. 
 	 \paragraph{Cameras}
 	 \begin{itemize}
 	 	\item \bfseries TPSCamera (Third Person Camera) 
 	 \end{itemize}
 	 
 	 Without the camera following the character, i cannot see what happened to him when he move outside the camera view, so in this section, i'll make a script to force the camera follow the character. To do that, let's implement transform.position = Target.position - transform.forward * distFromTarget , where the Target variable is the character object. The reason why i minus with transform.forward * distFromTarget is because to make sure that the camera always behind the character, not in front of him. 
 	 \begin{figure}[h]
 	 	\centering
 	 	\begin{minipage}{.4\textwidth}
 	 		\centering
 	 		\includegraphics[width=0.7\linewidth]{intructions/camera_space.png}
 	 			\centering
 	 		\captionof{figure}{Camera world space}
 	 		\label{fig:test6}
 	 	\end{minipage}
  		\begin{minipage}{.4\textwidth}
  			\centering
  			\includegraphics[width=1.2\linewidth]{intructions/my_camera_space.png}
  			\label{fig:test7}
  		\end{minipage}
 	 \end{figure}  
  	\\[0.05cm]
 	 The camera is specified in term of three parameters: {\bfseries EYE, AT}, and {\bfseries UP}, as show in the left one. EYE is the camera position , AT is the reference point the camera is pointing at, and UP usually is set to the y-axis of the world space. For example, in the right one, AT is set at the position a little bit above the character for not covering the center of the screen which i'm going to explain why later.
 	 After that, the camera needed to be rotated around with a mouse because i'd like to see what is happening around the character. And the same applied to the character controller script, but this time, i use Input.GetAxisRaw function instead of GetAxis is because i don't want the rotating to be smooth immiately as it can cause a short delay as GetAxisRaw will only return 0,-1 or 1 while GetAxis change gradually from 0 to 1 or 0 to -1.  
 	 Moreover, the function Lerp is used here instead of SmoothDamp due to the fact that the smoothdamp function adds the curve which can cause delay, for example, when i stop moving my mouse, the camera still moving, while Lerp only behaves linear. As now, there's still one more problem, on the mouseY rotation, the camera can be rotated 360 degree which is not i wanted, so i used the function Mathf.Clamp to limit the rotation of the mouseY axis, the minimum, maximum here is -40, 85 degree for the third person camera controller. In the end, to finish my TPS camera controller, i've modified the charactercontroller script a bit by adding cameraT.eulerAngles.y in the targetRotation where cameraT is the MainCamera(the TPS camera), to make sure that the camera will following the player correctly.
 	 
 	  \begin{itemize}
	 	\item \bfseries Camera Collision and Occlusion Detection  	 	
 	 \end{itemize}
 	 In some situations, like in the figure below, when the camera collide with the wall or the wall comes in between the character and the camera, they're called occlusion and collision. When the camera goes through the wall or the ground, it can cause shearing.
 	 \begin{figure}[h]
 	 	\centering
 	 	\begin{minipage}{.4\textwidth}
 	 		\centering
 	 		\includegraphics[width=1\linewidth]{intructions/camera_cover.png}
 	 		\centering
 	 		\captionof{figure}{Occlusion example}
 	 		\label{fig:test8}
 	 	\end{minipage}
  	\begin{minipage}{.4\textwidth}
  		\centering
  		\includegraphics[width=1\linewidth]{intructions/camera_collide.png}
  		\centering
  		\captionof{figure}{Collision example}
  		\label{fig:test9}
  	\end{minipage}
 	 \end{figure}
  \\[0.15cm]
  So, the goal here is to handle {\bfseries Collision} and {\bfseries Occlusion} with as little shearing as possible. Let's defined each one of these underlined terms really means. 
  \begin{figure}[h]
  	\centering
  	\begin{minipage}{.4\textwidth}
  		\centering
  		\includegraphics[width=1.3\linewidth]{intructions/Occlusion_camera.png}
  		\centering
  		\captionof{figure}{Occlusion}
  		\label{fig:test10}
  	\end{minipage}
  \end{figure}
  \\[0.15cm]
  Occlusion is when the game is running where the pillar or the wall comes in between the character and the camera view, the character is no where to be seen, and i need to move forward if that happen. Like the figure above, if camera isn't hitting anything but it's view is being obstructed so i have to move forward, the camera can see the player. 
  \begin{figure}[h]
  	\centering
  	\begin{minipage}{.4\textwidth}
  		\centering
  		\includegraphics[width=1.3\linewidth]{intructions/Collision_camera.png}
  		\centering
  		\captionof{figure}{Collision}
  		\label{fig:test11}
  	\end{minipage}
  \end{figure}
 \\[0.05cm]
  Collision is where the camera actually does hit something and the view is still obstructed so i still need to move the camera forward. 
  \\[0.05cm] From these, it's actually a problem that can be solved with one solution. I'm going to determine if the camera can see the player , if a point of the camera cannot see the player then the camera should be moved forward, whenever it's collision or occlusion, it's still the same issue. And shearing is when the camera go through the wall, i'm able to see through the wall too, to where i see the open space, even the skybox. It's really detract from the realism of the scene,so i have to limit the shearing as much as possible.  	
  \\[0.15cm]
  Before solving the problem, let's implement some definition of the Camera.
  \begin{figure}[h]
  	\centering
  	\begin{minipage}{.4\textwidth}
  		\centering
  		\includegraphics[width=1.7\linewidth]{intructions/Camera_overview.png}
  		\centering
  		\captionof{figure}{Occlusion}
  		\label{fig:test12}
  	\end{minipage}
  \end{figure} 
\\[0.05cm]
From the image above, 2 white lines are defines as a view angle. Inside the view angle, there's also a clip plane defined by the green rectangle, which is also the camera view. And at the center of the near clip plane is where the camera is pointing at, and at that point, i've drawn the crosshair, to make sure that this is the camera position. This is the position we the character can interact with the objects, NPCs so as i mention above in the character section, that's why the character cannot be at the middle of the near clip point. The second green rectangle is defined as the far clip plane, which determines how far out into the world the camera view can see. And between the two plane is view frustum, within this shape is what i can see in the game preview.
\\[0.05cm] To detect if the camera cannot see the player or the camera is collided with the objects, all i have to do is just a simple raycasting from the character to the camera, and what is raycasting, i'll explain in the next section. 
\begin{figure}[h]
	\centering
	\begin{minipage}{.4\textwidth}
		\centering
		\includegraphics[width=1.3\linewidth]{intructions/raycasting.png}
		\centering
		\captionof{figure}{Occlusion}
		\label{fig:test13}
	\end{minipage}
\end{figure} 
\\[0.05cm]
The best raycasting function to use here is Physics.LineCast, as it can return true if the camera hit something, vice versa. This function works in both Collision and Occlusion situation. Like the image above, i've drawn the LineCast between the character and the camera using the function Physics.LineCast, whenever the camera is behind the wall and the player or being collided with the wall, the line can still hit the objects, return true. 
\begin{lstlisting}
void CollisionCheck(Vector3 ReturnPoint)
{
	RaycastHit hit;
	if (Physics.Linecast(Target.position, ReturnPoint, out hit, collisionMask))
	{
		normal = hit.normal*wallPush;
		p = hit.point + normal;
		if (Vector3.Distance(Vector3.Lerp(transform.position, p, moveSpeed * Time.deltaTime), Target.position) <= EvenCloserDistance)
		{

		}
		else
		{
			transform.position = Vector3.Lerp(transform.position, p, moveSpeed * Time.deltaTime);
		}
		return;

	}
	transform.position = Vector3.Lerp(transform.position, ReturnPoint, returnSpeed * Time.deltaTime);
}
\end{lstlisting}
From the code above, the hit function is to return the information where the line hit, collisionMask determine which object layer should the line hit. After the line hit something, i've defined the function hit.normal to find the normal if the line hit the wall, and if it hit, the value will be 1, and the vector p is the position the camera should move. The reason why i do the normalize, add it with the hit.point is just to prevent the camera from clipping with the object when the camera start to move. Finally, to make sure that when occlusion happen, the camera still move to the correct position, be able to see the player, i measure the distance between the camera point and the player, if it's already smaller than a specific value than stop moving the camera, otherwise, it'll lerp to the correct position to see the player.  
\begin{itemize}
	\item \bfseries FPSCamera 	 	
\end{itemize}
	This camera represent like how everybody see the world in real life, in everyday life. The controller script for this one is basically the same as the TPScamera script i've written before, but the only difference is the FoV, it's a bit narrower than the FoV of TPScamera and the clamp angle. The camera position is right at the position of the eye's character, for making the gameplay looks more realistic. Moreover, i've written the script to allow the user to change between the FPS mode and TPS mode by pressing V. One more thing, to be able to synchronized 2 camera, all i do is copy almost all the code lines from TPSCamera script and then paste to the FPS one, so that 2 camera will rotate , move extracty at the same angle, same speed. 

\paragraph{RayCasting}
  A Raycast is conceptually like a beam that's fired from a point in space along a particular direction. Any object making contact with the beam can be detected and reported. With the ability to get information from the object that the beam hit, this is the main method for this project, as the character is able to interact with other objects in the virtual world. In Unity, raycast support both 2 dimension and 3 dimension, and for my project, i'll use 3d version for working with 3D space. There're two type of raycast that's usually used , Physics.Raycast and Physics.Linecast.
  \begin{itemize}
  	\item \bfseries Physics.Raycast(Vector3 origin,  Vector3 direction ,float maxDistance = Mathf.Infinity, out hit, , layerMask)	 	
  \end{itemize}
 	This type of function casts a ray from the original point, which is Vector3 origin into direction point (Vector3 direction) with the length of maxDistance. Normally, the maxDistance usually set to Mathf.Infinity for casting to the infinite length. This function will return true if the ray intersects with something, otherwise false. The forth variable, the out usually passed by hit, it provides the info about what it hit, like the distance from the origin to the point or the position where it hit, which is extractly what i need for my character to be able to interact, communicate with objects in the scene.
 	\begin{figure}[h]
 		
 		\begin{minipage}{1\textwidth}
 			\centering
 			\includegraphics[width=1\linewidth]{intructions/RayCast_example1.png}
 			\centering
 			\captionof{figure}{RayCast example}
 			\label{fig:test14}
 		\end{minipage}
 	
 	\end{figure} 
 	
 	For example, from the figure 13, the greens line is actually the ray drawn from raycast. In the scene above, this ray will provide the distance between my character and the object it hit by hit.distance. In this situation, as the player is far from this NPC, the text which i have defined before will not pop to the screen, but when i move close to the NPC, the text actually pop up: "[E] talk to NPC"
 	\begin{figure}[h]
 		
 			\begin{minipage}{1\textwidth}
 				\centering
 				\includegraphics[width=0.5\linewidth]{intructions/text_popup.png}
 				\centering
 				\captionof{figure}{Text popup}
 				\label{fig:test15}
 			\end{minipage}
 		\end{figure}
 	\\[0.05cm]
 	This is because i've declared a float variable , when hit.distance bigger than that variable, the text will not appear until hit.distance is smaller. This is the concept of communicating with objects in the scene. Besides, the out variable can be used to get the object name where the ray hit by using hit.collider.gameObject.name: 	
 	\noindent\begin{minipage}{0.3\textwidth}% adapt widths of minipages to your needs
 		\includegraphics[width=\linewidth]{intructions/objectname.png}
 	\end{minipage}%
 	\newpage
 	\begin{itemize}
 		\item \bfseries Physics.Linecast(Vector3 start,  Vector3 end , out hit, , layerMask)	 	
 	\end{itemize}
 		This function is pretty similar to the raycast function, although they both return true if the ray intersect with a collider, only recognize objects that have collider. The only difference between these two raycasting method is that the raycast only trigger the collider that starts outside of the origin object, and then intersects,  while the linecast is not, it will still trigger the collider of the beginning point. That's why i didn't use physics.raycast method for my camera collision detector, as physics.linecast trigger both the collider of two points.
 
	\paragraph{Bump mapping}
	In order to make the object surface looks more realistic, bump mapping is a technique in computer graphic to do that by simulating small displacements of the surface. However, the number of polygons does not increase. The modified surface heavily relied on light reflection, define how the light should shine on the surface.
	The method to perform bump mapping i'm going to use here will be divided into 3 stages: height map, normal map and occlusion map, the most important one in this method are height map and normal map. In this project, since i'll only generate bump map from a simple 2D texture, i'll not go too deep into this method, make it as simple as possible. This's the way how unity can perform bump mapping on the object surface.
	\begin{itemize}
		\item \bfseries Height map	 	
	\end{itemize}
		To be able to generate normal map, i'll need to create height map from the 2d texture first, to simulate the high of vertex, from that, i can simulate the dept, how the light can shine into the surface. A height map is used to faked depth or essentially height on areas of texture map, the larger the difference between the height and the black, the more the distance it will appear on the surface. For creating the height map, i'll use Photoshop to make it. The texture for generating  the height map is this brick texture, for example: 
		 \begin{figure}[h]
		 	\centering
		 	\begin{minipage}{.3\textwidth}
		 		\centering
		 		\includegraphics[width=0.8\linewidth]{intructions/Brick001.jpg}
		 		\centering
		 		\captionof{figure}{Original texture}
		 		\label{fig:test16}
		 	\end{minipage}
	 		\begin{minipage}{.3\textwidth}
	 			\centering
	 			\includegraphics[width=0.8\linewidth]{intructions/Brick001_height.png}
	 			\centering
	 			\captionof{figure}{Height map after creation}
	 			\label{fig:test17}
	 		\end{minipage}
 		\begin{minipage}{.3\textwidth}
 			\centering
 			\includegraphics[width=1.4\linewidth]{intructions/height_example.png}
 		\end{minipage}
 	\end{figure} 
 \\[0.02cm]
 Basically, the height image is the grey scale image with black and white range, so i'll convert the image to greyscale in Photoshop by selecting Image -> Adjustment -> Black and white. After creating the greyscale image, i'll try to modify the contrast by selecting a small circle at the bottom left, and choose level to adjust the black, white range, adjust the white area to become whiter, dark area to become even darker.
 \\[0.01cm]
 \noindent \begin{minipage}{0.21\textwidth}
 \includegraphics[width=\linewidth]{intructions/contrastAdjusting.png}
 \end{minipage}
  \\[0.2cm]
  If the height is in the range of [0,255], the lowest height 0 is colored in black, and the highest 255 is colored in white. As of now, there're still noises in the height map, i'll filter the noise by going to Filter -> Blur -> Gaussian Blur: \begin{minipage}{0.21\textwidth}
  	\includegraphics[width=\linewidth]{intructions/guassian_blur.png}
  \end{minipage}  Depends on each textures, but for this one, 3.5 pixels is enough to filter the noises, for not losing the detail like in the figure 16, so that i'll get better result when creating the normal map.  
\begin{itemize}
	\item \bfseries Normal map	 	
\end{itemize}
  A normal map is a special kind of texture that allow you to add surface detail such as bumps, grooves, and scratches to a model which catch the light as if they are represented by real geometry, it contains surface normals. 
  \begin{figure}[h]
  	\begin{minipage}{.45\textwidth}
  		\centering
  		\includegraphics[width=1\linewidth]{intructions/surface_normal.png}
  		\centering
  		\captionof{figure}{Surface normal, across 3 polygons}
  		\label{fig:test18}
  	\end{minipage}
  	\begin{minipage}{.4\textwidth}
  		\centering
  		\includegraphics[width=1.6\linewidth]{intructions/surface_normal_across.png}
  		\centering
  		\captionof{figure}{Normal mapping across three polygons}
  		\label{fig:test179}
  	\end{minipage}
  \end{figure}
 \\[0.01cm]
 The simplest way is to compute the surface normal from the height map as i created above at the point  (x,y,h(x,y)). Here, h represents the discrete height function, and x,y represent as the pixel coordinate. 
 \begin{figure}[h]
 	\begin{minipage}{1\textwidth}
 		\centering
 		\includegraphics[width=0.8\linewidth]{intructions/normal_map_calculation.png}
 		\centering
 	\end{minipage}
 \end{figure} 
\\[0.01cm]
In here, the partial derivatives are need for computing the normal: $\frac{\partial h}{\partial x}$ and $\frac{\partial h}{\partial y}$. They are defined by the height store at four neighbors of the pixel coordinate: \{(x + 1,y),(x $- 1$,y),(x,y + 1),(x,y $- 1$)\}. \\[0.01cm]The equation of these partial derivatives are: \begin{minipage}{1\textwidth}
	\scalebox{1.3} {$\frac{\partial h}{\partial x} = \dfrac{h(x + 1,y) - h(x - 1,y)}{2}$ }
	\\[0.04cm]
	\scalebox{1.3} {$\frac{\partial h}{\partial y} = \dfrac{h(x,y + 1) - h(x,y - 1)}{2}$ }
\end{minipage}
\\[0.2cm] After calculating these derivatives, the cross product is ({$-\frac{\partial h}{\partial x}$, $-\frac{\partial h}{\partial y}$, $1$), which is normalized and store at (x,y). In the C\# function, this can be implemented as: \begin{lstlisting}
	private Texture2D GenerateNormal(Texture2D source)
	{
		normalTexture = new Texture2D(source.width, source.height, TextureFormat.ARGB32, true);
		for (int y = 0; y < normalTexture.height; y++)
		{
			for (int x = 0; x < normalTexture.width; x++)
			{
				xHeight = (GetColor((source.GetPixel(x+1, y) - source.GetPixel(x - 1, y)))-1)*0.5f;
				yHeight = (GetColor((source.GetPixel(x, y + 1) - source.GetPixel(x, y - 1)))-1)*0.5f;
				normalTexture.SetPixel(x, y, new Color(-xHeight, -yHeight, 1.0f, 1.0f));
	
			}
		}
		normalTexture.Apply();
		System.IO.File.WriteAllBytes("Assets/Sample001_n.png", normalTexture.EncodeToPNG());
		return normalTexture;
	}
	\end{lstlisting}
 Where x is the width and y is the height of image. The reason i minus the derivative functions with 1 is because i'm trying to get the color value in the height map , and the normal value is calculate by this equation: $ Normal = (2.Color) - 1 $ so $ Color = \frac{Normal + 1}{2}$. And the SetPixel function is set by the negative derivative, so i should minus the normal with 1 instead of plus. With the normal map, the surface will look like in the picture below, as how the light shines on the surface.
 
 \begin{figure}[h]
 	\centering
 	\begin{minipage}{.3\textwidth}
 		\centering
 		\includegraphics[width=0.5\linewidth]{intructions/Brick001_n.png}
 		\centering
 		\captionof{figure}{The normal map that i created}
 		\label{fig:test19}
 	\end{minipage}
 	\begin{minipage}{.33\textwidth}
 		\centering
 		\includegraphics[width=1\linewidth]{intructions/without_normalmap.png}
 		\centering
 		\captionof{figure}{Without normal map}
 		\label{fig:test20}
 	\end{minipage}
 	\begin{minipage}{.33\textwidth}
 		\centering
 		\includegraphics[width=1\linewidth]{intructions/with_normalmap.png}
 		\captionof{figure}{With normal map applied}
 		\label{fig:test21}
 	\end{minipage}
 \end{figure} 

\begin{itemize}
	\item \bfseries Occlusion map	 	
\end{itemize}
The occlusion map will determine which area are inherently darkened to simulate shadow, anywhere is white will not receive darkening, anywhere that's black or gray will receive darkening. For this map, as i can't find a proper way, algorithm to actually generate a true occlusion map from just a 2d texture, so i decided to write a C\# script outside unity to make an occlusion map, because in unity, there's no System.Draw library for editing the texture.
\newpage
\begin{figure}[h]
	\centering
	\raisebox{-25mm}[0pt][0pt]{
		\begin{minipage}{.4\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{intructions/Brick001_n.png}
			\centering
			\captionof{figure}{The normal map}
			\label{fig:test22}
		\end{minipage}
		\begin{minipage}{.4\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{intructions/Brick001_occ.png}
			\centering
			\captionof{figure}{Occlusion map after i created using C\# script}
			\label{fig:test23}
		\end{minipage}
	}
\end{figure}
\vspace{5.5cm}
From 2 images above, the purple color, with RGB value of (0.5, 0.5, 1) as base color, with z direction being up as the blue channel. The reason the values in the textures are treated as having been halved, with 0.5 is because from the equation above i mentioned in above section, is divided by 2 when normalizing the height value. In the normal map texture, i found out that the bluey color and the deep purple one is actually the direction of the light shining on the surface of the texture, as the deep purple is behaved as opposite direction with the bluey one. When convert the texture into grey-scale image by getting red, green, blue color and divided by 3, the light blue area will turn into white color, the purple one will turn into dark-grey color, which behave almost like an occlusion map. To make sure that the surface(or the light grey area) will not too dark when applying occlusion map, i've lighten up the texture a bit by using ControlPaint.Light function just like in figure 23. 
\begin{figure}[h]
	\begin{minipage}{.4\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{intructions/without_occlusion.png}
		\centering
		\captionof{figure}{without occlusion}
		\label{fig:test24}
	\end{minipage}
	\begin{minipage}{.45\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{intructions/with_occlusion.png}
		\centering
		\captionof{figure}{with occlusion being applied}
		\label{fig:test25}
	\end{minipage}
 
\end{figure}
It's quite hard to see the difference, but in the right side, at the bottom of the brick, it's darker than the left side as like there's a shadow there

\paragraph{Lighting}
Lights are essential part of every scene. In unity, while the mesh, textures define the shape, the look of an object, the lights will define the color, the mood of a 3D environment. Lights can be added into the scene just by go to {\bfseries GameObject -> Light} menu . In the menu, i can choose any form of light i want, once the light has been added into the scene, i can adjust the properties of the light to the way i want in the inspector, there are many different options within the Light Component. 
\newpage
\begin{figure}[h]
	\centering
	\raisebox{-25mm}[0pt][0pt]{
		\begin{minipage}{.4\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{intructions/choose_light.png}
			\centering
			\captionof{figure}{Light section}
			\label{fig:test26}
		\end{minipage}
		\begin{minipage}{.45\textwidth}
			\centering
			\includegraphics[width=1.2\linewidth]{intructions/light_properties.png}
			\centering
			\captionof{figure}{Light properties}
			\label{fig:test27}
		\end{minipage}
	}
\end{figure}
  \vspace{5.5cm}
  Just by simply changing the color of a light, this can give a whole different mood to the scene
  \begin{figure}[h]
  	\begin{minipage}{1\textwidth}
  		\centering
  		\includegraphics[width=0.8\linewidth]{intructions/change_light_color.png}
  		\centering
  		\captionof{figure}{Changing light color}
  		\label{fig:test28}
  	\end{minipage}
  \end{figure}
 However, lighting in unity also includes realtime GI or global illumination, illumination means lighten up. GI is a general name for a group of methods and algorithms to add more realistic lighting into the scene, like how the like is bounce off surface onto other surfaces, instead of just limited to the light that hit the surface directly. 
 \begin{figure}[h]
 	\begin{minipage}{1\textwidth}
 		\centering
 		\includegraphics[width=0.8\linewidth]{intructions/light_categlories.png}
 		\centering
 		\end{minipage}
 	\end{figure}
 \\[0.05cm]
 In computer graphic, this act of light bouncing around into two categories: direct illumination and indirect illumination. Direct lighting is when light gets send out from a light source, bounces and then reaches the eye (or the camera). Indirect lightning is when light bounces off multiple surfaces before hitting the eye, light will keep bouncing until it get fully absorbed. From the figure 26, all the type of light from Light section, like directional light, point light, etc.. they are all direct lights because they're all light source. When defining about indirect lighting, which mean every kind of light that's not directly point anywhere from the scenes, but by going to Window -> Rendering -> Lighting and look at environment lighting section, that's a great example of indirect lighting and so GI. And this's actually when GI basically comes to play, and GI will take care of direct lighting and indirect lighting that reflect from surfaces meaning it helps illuminating the scene more realistically. 
 \begin{figure}[h]
 	
 \end{figure}
 Without GI enabled, i cannot see any light bounce at all, even the light source is directly pointing at the sphere. With GI enabled, the room is now become a bit brighter , with a bit of light bouncing to the ceiling. The way the GI works, is that when the light travel through the gap that seen from the image above, and then the photon that travel from the light source will carry the values of surface that hit to the next surface. And with that way, that's can explain the reason why i can see the green light on the ceiling, that's the light that inherit from the green cube and blue sphere. 
  \begin{figure}[h]
 	\begin{minipage}{1\textwidth}
 		\centering
 		\includegraphics[width=0.8\linewidth]{intructions/How_light_works.png}
 		\centering
 	\end{minipage}
 \end{figure}
\\[0.05cm]
 In unity, it provides two techniques for pre computing GI: Baked GI and pre-computed real-time GI. The Baked one is pre-computed and will save a ton of processing power, and this only work if all the objects, light-source in the scene are static, and real-time is extractly lighting calculated in real-time, per frame, means that the light, shadow position can be changed overtime when an object or the light source is moving. whenever to change the mode to baked or real-time, this depend on each object, light source. For the sunlight (the light that shines the whole scene), i set the mode to real-time because there're still some moving objects like the player, the NPC, etc... and for the light like the room light, i set it to baked mode. A combination of baked and realtime lights can help maintain performance
 To enable GI mode, all i have to do is go to window -> rendering -> lighting setting, here's the properties that i set for making the light looks more realistic. The reason i have to enable both realtime and baked GI is because i have both baked and realtime light in the scene.
 \newpage
 \begin{figure}[h]
 	\raisebox{-50mm}[0pt][0pt]{
 	\begin{minipage}{1\textwidth}
 		\centering
 		\includegraphics[width=0.4\linewidth]{intructions/lighting_setting.png}
 		\captionof{figure}{Light setting}
 		\centering
 	\end{minipage}
}
 \end{figure}
\vspace{125mm}
 However, there're currently limitations of GI so both baked and pre-computed real-time GI in unity have the limitation that only objects in the scene that set to static can be included that means moving objects cannot bounced light onto other objects and vice versa, but still they can pick up bounced light from static object using light probes. Light Probes can be created by going to GameObject -> Light -> Light probe groups.
 
 \paragraph{Shader}
   	
  	\end{document}
	
 	 
 	 	
 	 
 	 
 	 